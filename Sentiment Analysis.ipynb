{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string, os, warnings\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = data.review[:40000]\n",
    "train_sentiments = data.sentiment[:40000]\n",
    "\n",
    "test_reviews = data.review[40000:]\n",
    "test_sentiments = data.sentiment[40000:]\n",
    "\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "data['review'] = data['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "data['review'] = data['review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "data['review'] = data['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text, is_lower_case = False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "data['review'] = data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_reviews = data.review[:40000]\n",
    "norm_train_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test_reviews = data.review[40000:]\n",
    "norm_test_reviews[45005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df = 0, max_df = 1, binary = False, ngram_range = (1,3))\n",
    "\n",
    "cv_train_reviews = cv.fit_transform(norm_train_reviews)\n",
    "cv_test_reviews = cv.transform(norm_test_reviews)\n",
    "\n",
    "print('BOW_cv_train:', cv_train_reviews.shape)\n",
    "print('BOW_cv_test:', cv_test_reviews.shape)\n",
    "#vocab=cv.get_feature_names()-toget feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(min_df = 0, max_df = 1, use_idf = True, ngram_range = (1,3))\n",
    "\n",
    "tv_train_reviews = tv.fit_transform(norm_train_reviews)\n",
    "tv_test_reviews = tv.transform(norm_test_reviews)\n",
    "\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "\n",
    "sentiment_data = lb.fit_transform(data['sentiment'])\n",
    "print(sentiment_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiments = sentiment_data[:40000]\n",
    "test_sentiments = sentiment_data[40000:]\n",
    "\n",
    "print(train_sentiments)\n",
    "print(test_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty = 'l2', max_iter = 500, C = 1, random_state = 42)\n",
    "\n",
    "lr_cv = lr.fit(cv_train_reviews, train_sentiments)\n",
    "print(lr_cv)\n",
    "\n",
    "lr_tfidf = lr.fit(tv_train_reviews, train_sentiments)\n",
    "print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_predict = lr.predict(cv_test_reviews)\n",
    "print(lr_cv_predict)\n",
    "\n",
    "lr_tfidf_predict = lr.predict(tv_test_reviews)\n",
    "print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_score = accuracy_score(test_sentiments, lr_cv_predict)\n",
    "print(\"lr_cv_score :\", lr_cv_score)\n",
    "\n",
    "lr_tfidf_score = accuracy_score(test_sentiments, lr_tfidf_predict)\n",
    "print(\"lr_tfidf_score :\", lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_report = classification_report(test_sentiments, lr_cv_predict, target_names = ['Positive','Negative'])\n",
    "print(lr_cv_report)\n",
    "\n",
    "lr_tfidf_report = classification_report(test_sentiments, lr_tfidf_predict, target_names = ['Positive','Negative'])\n",
    "print(lr_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_cv = confusion_matrix(test_sentiments, lr_cv_predict, labels = [1,0])\n",
    "print(cm_cv)\n",
    "\n",
    "cm_tfidf = confusion_matrix(test_sentiments, lr_tfidf_predict, labels = [1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SGDClassifier(loss = 'hinge', max_iter = 500, random_state = 42)\n",
    "\n",
    "svm_cv = svm.fit(cv_train_reviews, train_sentiments)\n",
    "print(svm_cv)\n",
    "\n",
    "svm_tfidf = svm.fit(tv_train_reviews, train_sentiments)\n",
    "print(svm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_cv_predict = svm.predict(cv_test_reviews)\n",
    "print(svm_cv_predict)\n",
    "\n",
    "svm_tfidf_predict = svm.predict(tv_test_reviews)\n",
    "print(svm_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_cv_score = accuracy_score(test_sentiments, svm_cv_predict)\n",
    "print(\"svm_cv_score :\", svm_cv_score)\n",
    "\n",
    "svm_tfidf_score = accuracy_score(test_sentiments, svm_tfidf_predict)\n",
    "print(\"svm_tfidf_score :\", svm_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_cv_report = classification_report(test_sentiments, svm_cv_predict, target_names = ['Positive','Negative'])\n",
    "print(svm_cv_report)\n",
    "\n",
    "svm_tfidf_report = classification_report(test_sentiments, svm_tfidf_predict, target_names = ['Positive','Negative'])\n",
    "print(svm_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_cv = confusion_matrix(test_sentiments, svm_cv_predict, labels = [1,0])\n",
    "print(cm_cv)\n",
    "\n",
    "cm_tfidf = confusion_matrix(test_sentiments, svm_tfidf_predict, labels = [1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb_cv = mnb.fit(cv_train_reviews, train_sentiments)\n",
    "print(mnb_cv)\n",
    "\n",
    "mnb_tfidf = mnb.fit(tv_train_reviews, train_sentiments)\n",
    "print(mnb_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_cv_predict = mnb.predict(cv_test_reviews)\n",
    "print(mnb_cv_predict)\n",
    "\n",
    "mnb_tfidf_predict = mnb.predict(tv_test_reviews)\n",
    "print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_cv_score = accuracy_score(test_sentiments, mnb_cv_predict)\n",
    "print(\"mnb_cv_score :\", mnb_cv_score)\n",
    "\n",
    "mnb_tfidf_score = accuracy_score(test_sentiments, mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\", mnb_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_cv_report = classification_report(test_sentiments, mnb_cv_predict, target_names = ['Positive','Negative'])\n",
    "print(mnb_cv_report)\n",
    "\n",
    "mnb_tfidf_report = classification_report(test_sentiments, mnb_tfidf_predict, target_names = ['Positive','Negative'])\n",
    "print(mnb_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_cv = confusion_matrix(test_sentiments, mnb_cv_predict, labels = [1,0])\n",
    "print(cm_cv)\n",
    "\n",
    "cm_tfidf = confusion_matrix(test_sentiments, mnb_tfidf_predict, labels = [1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of positive and negative words in cloud format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "positive_text = norm_train_reviews[1]\n",
    "WC = WordCloud(width = 1000, height = 500, max_words = 500, min_font_size = 5)\n",
    "positive_words = WC.generate(positive_text)\n",
    "plt.imshow(positive_words, interpolation = 'bilinear')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "negative_text = norm_train_reviews[8]\n",
    "WC = WordCloud(width = 1000, height = 500, max_words = 500, min_font_size = 5)\n",
    "negative_words = WC.generate(negative_text)\n",
    "plt.imshow(negative_words, interpolation = 'bilinear')\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
